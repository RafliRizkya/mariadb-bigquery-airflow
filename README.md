# MariaDB to BigQuery ETL with Airflow

This project provides an **ETL (Extract, Transform, Load) pipeline** to transfer data from **MariaDB** to **Google BigQuery**, orchestrated using **Apache Airflow**. It uses Docker for containerization and includes custom operators for data ingestion.

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ config
â”‚   â””â”€â”€ tables.yaml              # Table mapping and configuration
â”‚
â”œâ”€â”€ credentials
â”‚   â””â”€â”€ service-account.json      # GCP Service Account key for BigQuery
â”‚
â”œâ”€â”€ dags
â”‚   â””â”€â”€ daily_etl_dag.py          # Main Airflow DAG for daily ETL
â”‚
â”œâ”€â”€ logs                          # Airflow logs
â”‚
â”œâ”€â”€ scripts
â”‚   â””â”€â”€ setup.py                  # Setup / initialization script
â”‚
â”œâ”€â”€ sql                           # SQL query files
â”‚
â”œâ”€â”€ src
â”‚   â””â”€â”€ operators
â”‚       â””â”€â”€ mariadb_to_bigquery_operator.py  # Custom Airflow operator
â”‚
â”œâ”€â”€ utils
â”‚   â””â”€â”€ config_loader.py          # Utility for loading configs
â”‚
â”œâ”€â”€ .env.example                  # Example environment variables
â”œâ”€â”€ docker-compose.yml            # Docker services definition
â”œâ”€â”€ Dockerfile                    # Custom Airflow image definition
â”œâ”€â”€ README.md                     # Project documentation
â””â”€â”€ requirements.txt              # Python dependencies
```

## âš™ï¸ Prerequisites

- **Docker & Docker Compose** installed on your machine
- **Google Cloud Service Account Key** with BigQuery access
  - Save the JSON key inside `credentials/service-account.json`
- **MariaDB Database** with accessible host/port
- Update `.env` file with your configuration

## ğŸš€ Setup & Run

### 1. Clone the repository

```bash
git clone <repo_url>
cd mariadb-bigquery-etl
```

### 2. Copy environment file

```bash
cp .env.example .env
```

### 3. Update environment variables inside `.env`

Example:
```bash
MARIADB_HOST=mariadb
MARIADB_USER=root
MARIADB_PASSWORD=example
MARIADB_DB=mydb

GCP_PROJECT_ID=my-gcp-project
BIGQUERY_DATASET=my_dataset
```

### 4. Start Airflow with Docker

```bash
docker-compose up -d --build
```

### 5. Access Airflow UI

- **URL**: http://localhost:8080
- **Default login**: `airflow / airflow`

## ğŸ“Š Workflow

1. **Extract** data from MariaDB
2. **Transform** if necessary (via SQL / Python scripts)
3. **Load** data into BigQuery tables

The main DAG is defined in:
```
dags/daily_etl_dag.py
```

Custom operator for MariaDB â†’ BigQuery is located at:
```
src/operators/mariadb_to_bigquery_operator.py
```

## ğŸ“Œ Notes

- Table configurations (e.g., source â†’ destination mapping) are in `config/tables.yaml`
- BigQuery authentication requires the service account key stored in `credentials/service-account.json`

## ğŸ‘¤ Author

**Rafli Rizkya Sakti Nugraha** | Data Analyst Entry
